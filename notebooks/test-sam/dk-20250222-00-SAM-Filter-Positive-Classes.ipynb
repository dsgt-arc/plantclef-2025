{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b9373927-0615-41d3-9eac-3dd6b5e25fbe",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sat Feb 22 17:45:27 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 555.42.02              Driver Version: 555.42.02      CUDA Version: 12.5     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  Quadro RTX 6000                On  |   00000000:AF:00.0 Off |                  Off |\n",
      "| 33%   29C    P8              3W /  260W |    7264MiB /  24576MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A   1688746      C   ...atch1/2/dkhattak6/.venv/bin/python3       7260MiB |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bad2277-9669-494f-ae32-42df8854d59b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HOME: /storage/scratch1/2/dkhattak6\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "HOME = os.getcwd()\n",
    "print(\"HOME:\", HOME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aa9e0f0b-4b90-4d70-8a5f-37712e5d89fd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "!echo $PYSPARK_PYTHON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6a4d281a-8063-4781-910b-66dce618fe59",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.12.5 (main, Dec 19 2024, 14:46:22) [GCC 11.4.1 20231218 (Red Hat 11.4.1-3)]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6e2b0788-4101-4d69-ade3-98b186490f47",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "!echo $PYSPARK_PYTHON $PYSPARK_DRIVER_PYTHON\n",
    "os.environ[\"PYSPARK_PYTHON\"] = sys.executable\n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON\"] = sys.executable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "75d77ce1-338d-4081-96a5-d2d7dee5829e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/storage/scratch1/2/dkhattak6/.venv/bin/python3 /storage/scratch1/2/dkhattak6/.venv/bin/python3\n"
     ]
    }
   ],
   "source": [
    "!echo $PYSPARK_PYTHON $PYSPARK_DRIVER_PYTHON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "00021959-5e44-4097-b3f7-681d45d46c10",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch._C\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4dac8fcc-0ce9-4e8a-922b-3e20d6ec7e75",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/cuda\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.cpp_extension import CUDA_HOME\n",
    "\n",
    "print(CUDA_HOME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d9da30db-154c-4967-a5d9-9fdb9f947766",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<module 'torch._C' from '/storage/scratch1/2/dkhattak6/.venv/lib/python3.12/site-packages/torch/_C.cpython-312-x86_64-linux-gnu.so'>\n"
     ]
    }
   ],
   "source": [
    "print(torch._C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "261f7f88-2bd5-4224-a7cc-ef2958f480af",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/storage/scratch1/2/dkhattak6/weights/sam_vit_h_4b8939.pth ; exist: True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "SAM_CHECKPOINT_PATH = os.path.join(HOME, \"weights\", \"sam_vit_h_4b8939.pth\")\n",
    "print(SAM_CHECKPOINT_PATH, \"; exist:\", os.path.isfile(SAM_CHECKPOINT_PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "54d6c215-dff8-4185-ba75-b0afd8fea69c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/storage/scratch1/2/dkhattak6/weights/groundingdino_swint_ogc.pth ; exist: True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "GROUNDING_DINO_CHECKPOINT_PATH = os.path.join(\n",
    "    HOME, \"weights\", \"groundingdino_swint_ogc.pth\"\n",
    ")\n",
    "print(\n",
    "    GROUNDING_DINO_CHECKPOINT_PATH,\n",
    "    \"; exist:\",\n",
    "    os.path.isfile(GROUNDING_DINO_CHECKPOINT_PATH),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "378e8d5e-b5c8-4b4b-92e3-f8e5676e5235",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/storage/scratch1/2/dkhattak6/GroundingDINO/groundingdino/config/GroundingDINO_SwinT_OGC.py ; exist: True\n"
     ]
    }
   ],
   "source": [
    "GROUNDING_DINO_CONFIG_PATH = os.path.join(\n",
    "    HOME, \"GroundingDINO/groundingdino/config/GroundingDINO_SwinT_OGC.py\"\n",
    ")\n",
    "print(\n",
    "    GROUNDING_DINO_CONFIG_PATH, \"; exist:\", os.path.isfile(GROUNDING_DINO_CONFIG_PATH)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "86eb4b0c-0df8-4b2f-8a77-65ae6452ca89",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, BinaryType, StringType, MapType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "29f93502-5822-4b0c-ae5b-e9eb9ea1e56e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.8\n"
     ]
    }
   ],
   "source": [
    "print(torch.version.cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "15613aef-b46b-4fd1-ac33-aa94af65b664",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/02/22 17:46:50 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PySpark Version: 3.5.4\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName(\"Print PySpark Version\").getOrCreate()\n",
    "print(\"PySpark Version:\", spark.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2b4f925d-bc26-485f-a7ae-40c7e8a89384",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "print(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "615103bb-889c-4af0-b740-ed2ce822e36e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/storage/scratch1/2/dkhattak6/GroundingDINO\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/storage/scratch1/2/dkhattak6/.venv/lib/python3.12/site-packages/timm-1.0.14-py3.12.egg/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n",
      "/storage/scratch1/2/dkhattak6/.venv/lib/python3.12/site-packages/torch/functional.py:507: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3549.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final text_encoder_type: bert-base-uncased\n"
     ]
    }
   ],
   "source": [
    "%cd {HOME}/GroundingDINO\n",
    "\n",
    "from groundingdino.util.inference import Model\n",
    "\n",
    "grounding_dino_model = Model(\n",
    "    model_config_path=GROUNDING_DINO_CONFIG_PATH,\n",
    "    model_checkpoint_path=GROUNDING_DINO_CHECKPOINT_PATH,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4d00ab25-67d1-410e-b691-014080e33523",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7fb03f4-f2a5-47c7-b9c0-37638387af50",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from segment_anything import sam_model_registry, SamPredictor\n",
    "\n",
    "SAM_ENCODER_VERSION = \"vit_h\"\n",
    "sam = sam_model_registry[SAM_ENCODER_VERSION](checkpoint=SAM_CHECKPOINT_PATH).to(\n",
    "    device=DEVICE\n",
    ")\n",
    "sam_predictor = SamPredictor(sam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a0b717c1-7f7a-40e4-971c-b348754582b2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "CLASSES = [\n",
    "    \"leaf\",\n",
    "    \"flower\",\n",
    "    \"sand\",\n",
    "    \"wood\",\n",
    "    \"stone\",\n",
    "    \"tape\",\n",
    "    \"plant\",\n",
    "    \"tree\",\n",
    "    \"rock\",\n",
    "    \"vegetation\",\n",
    "]\n",
    "BOX_TRESHOLD = 0.15\n",
    "TEXT_TRESHOLD = 0.1\n",
    "include_class_ids = {0, 1, 6, 7, 9}  # only include positive classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7fc576a2-8667-42bf-b3f7-56af23511aa8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "\n",
    "def enhance_class_name(class_names: List[str]) -> List[str]:\n",
    "    return [f\"all {class_name}s\" for class_name in class_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7fad9984-beb4-4cda-afb7-0ce61225312c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from segment_anything import SamPredictor\n",
    "import cv2\n",
    "import supervision as sv\n",
    "\n",
    "\n",
    "def segment(\n",
    "    sam_predictor: SamPredictor, image: np.ndarray, xyxy: np.ndarray\n",
    ") -> np.ndarray:\n",
    "    sam_predictor.set_image(image)\n",
    "    result_masks = []\n",
    "    for box in xyxy:\n",
    "        masks, scores, logits = sam_predictor.predict(box=box, multimask_output=True)\n",
    "        index = np.argmax(scores)\n",
    "        result_masks.append(masks[index])\n",
    "    return np.array(result_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "32b6bba4-8dd8-4442-a917-bbdbe941b1b3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "\n",
    "def image_to_bytes(image_array, ext=\".png\"):\n",
    "    \"\"\"\n",
    "    Encode an image or mask array to bytes using OpenCV.\n",
    "    Use PNG for masks (lossless) and JPG for the original image.\n",
    "    \"\"\"\n",
    "    success, buffer = cv2.imencode(ext, image_array)\n",
    "    if not success:\n",
    "        raise ValueError(\"Image encoding failed!\")\n",
    "    return buffer.tobytes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a7b8753-8eb6-4965-be7c-6ff54a08563f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51aeff96ecac4a50b7412fca9dfe2678",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1695 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "from pathlib import Path\n",
    "\n",
    "counter = 0\n",
    "IMAGES_DIRECTORY = os.path.join(HOME, \"data\")\n",
    "HOMEPATH = Path(os.path.expanduser(\"~/shared/plantclef/data\"))\n",
    "IMAGES_DIRECTORY = os.path.join(HOMEPATH, \"test_2024/images\")\n",
    "IMAGES_EXTENSIONS = [\"jpg\"]\n",
    "MASK_IMAGE_PATH = f\"{HOME}/masks_2024/\"\n",
    "records = []\n",
    "images = {}\n",
    "annotations = {}\n",
    "image_paths = sv.list_files_with_extensions(\n",
    "    directory=IMAGES_DIRECTORY, extensions=IMAGES_EXTENSIONS\n",
    ")\n",
    "\n",
    "for image_path in tqdm(image_paths):\n",
    "    image_name = image_path.name\n",
    "    image_path = str(image_path)\n",
    "    image = cv2.imread(image_path)\n",
    "\n",
    "    detections = grounding_dino_model.predict_with_classes(\n",
    "        image=image,\n",
    "        classes=enhance_class_name(class_names=CLASSES),\n",
    "        box_threshold=BOX_TRESHOLD,\n",
    "        text_threshold=TEXT_TRESHOLD,\n",
    "    )\n",
    "    detections = detections[detections.class_id is not None]\n",
    "    detections.mask = segment(\n",
    "        sam_predictor=sam_predictor,\n",
    "        image=cv2.cvtColor(image, cv2.COLOR_BGR2RGB),\n",
    "        xyxy=detections.xyxy,\n",
    "    )\n",
    "    images[image_name] = image\n",
    "    annotations[image_name] = detections\n",
    "    titles = [CLASSES[class_id] for class_id in detections.class_id]\n",
    "    merged_masks = []\n",
    "    merged_titles = []\n",
    "    per_class_mask_dict = {}\n",
    "\n",
    "    grouped = {}  # {class_id: [mask1, mask2, ...]}\n",
    "    for mask, class_id in zip(detections.mask, detections.class_id):\n",
    "        if class_id not in grouped:\n",
    "            grouped[class_id] = []\n",
    "        grouped[class_id].append(mask)\n",
    "\n",
    "    for class_id, masks in grouped.items():\n",
    "        if class_id in include_class_ids:\n",
    "            if len(masks) > 0:\n",
    "                merged_mask = np.any(\n",
    "                    np.stack(masks, axis=0), axis=0\n",
    "                )  # Basically doing a Logical OR over the masks\n",
    "                merged_masks.append(merged_mask)\n",
    "                merged_titles.append(CLASSES[class_id])\n",
    "                per_class_mask_dict[CLASSES[class_id]] = (\n",
    "                    merged_mask.astype(np.uint8)\n",
    "                ) * 255\n",
    "\n",
    "    if len(merged_masks) > 0:\n",
    "        final_mask = np.any(np.stack(merged_masks, axis=0), axis=0)\n",
    "        final_mask_uint8 = (final_mask.astype(np.uint8)) * 255\n",
    "        segmented_image = cv2.bitwise_and(image, image, mask=final_mask_uint8)\n",
    "    else:\n",
    "        segmented_image = image\n",
    "        # print(\"No masks found for image \" + str(image_name))\n",
    "        final_mask_uint8 = None\n",
    "        counter += 1\n",
    "        per_class_mask_dict = {}\n",
    "\n",
    "    cv2.imwrite(MASK_IMAGE_PATH + str(image_name), segmented_image)\n",
    "    original_bytes = image_to_bytes(image, ext=\".jpg\")\n",
    "    final_mask_bytes = (\n",
    "        image_to_bytes(final_mask_uint8, ext=\".png\")\n",
    "        if final_mask_uint8 is not None\n",
    "        else None\n",
    "    )\n",
    "\n",
    "    class_masks_bytes = {}\n",
    "    for cls, mask in per_class_mask_dict.items():\n",
    "        class_masks_bytes[cls] = image_to_bytes(mask, ext=\".png\")\n",
    "\n",
    "    # Create a record for this image\n",
    "    record = (image_name, original_bytes, final_mask_bytes, class_masks_bytes)\n",
    "    records.append(record)\n",
    "\n",
    "print(\"Could not create masks for \" + str(counter) + \" images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2f7e9894-7f34-40a4-bd40-3da74df1e7a2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/storage/home/hcoda1/2/dkhattak6/shared/plantclef/data/masks_test_2024/\n"
     ]
    }
   ],
   "source": [
    "print(MASK_IMAGE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "03e93af6-2ca8-41f5-aa2a-bb278f2b3840",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "schema = StructType(\n",
    "    [\n",
    "        StructField(\"image_name\", StringType(), False),\n",
    "        StructField(\"image\", BinaryType(), False),\n",
    "        StructField(\"final_mask\", BinaryType(), True),\n",
    "        StructField(\"class_masks\", MapType(StringType(), BinaryType()), True),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e1b799-f4cf-4d65-8583-b558080fb1cf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"SegmentedImageStorage\").getOrCreate()\n",
    "\n",
    "\n",
    "schema = StructType(\n",
    "    [\n",
    "        StructField(\"image_name\", StringType(), False),\n",
    "        StructField(\"image\", BinaryType(), False),\n",
    "        StructField(\"final_mask\", BinaryType(), True),\n",
    "        StructField(\"class_masks\", MapType(StringType(), BinaryType()), True),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Create the DataFrame from the list of records\n",
    "df = spark.createDataFrame(records, schema=schema)\n",
    "image_final_df = df.select(\n",
    "    F.col(\"image_name\"), F.col(\"image\"), F.col(\"final_mask\"), F.col(\"class_masks\")\n",
    ").repartition(500)\n",
    "image_final_df.write.mode(\"overwrite\").parquet(\"segmented_images.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "46838cbd-a577-4b96-9b25-e10b115008bb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- image_name: string (nullable = true)\n",
      " |-- path: string (nullable = true)\n",
      " |-- data: binary (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+\n",
      "|          image_name|                path|                data|\n",
      "+--------------------+--------------------+--------------------+\n",
      "|CBN-Pla-B3-201907...|/test/data/PlantC...|[FF D8 FF E0 00 1...|\n",
      "|CBN-PdlC-E5-20180...|/test/data/PlantC...|[FF D8 FF E0 00 1...|\n",
      "|CBN-PdlC-B1-20140...|/test/data/PlantC...|[FF D8 FF E0 00 1...|\n",
      "|CBN-Pla-D4-201507...|/test/data/PlantC...|[FF D8 FF E0 00 1...|\n",
      "|CBN-PdlC-F3-20190...|/test/data/PlantC...|[FF D8 FF E0 00 1...|\n",
      "+--------------------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "root = Path(os.path.expanduser(\"~\"))\n",
    "data_path = f\"{root}/shared/plantclef/data/parquet/\"\n",
    "\n",
    "# Define the path to the train and test parquet files\n",
    "\n",
    "test_path = f\"{data_path}/test\"\n",
    "\n",
    "# Read the parquet files into a spark DataFrame\n",
    "\n",
    "test_df = spark.read.parquet(test_path)\n",
    "\n",
    "# Show the data\n",
    "test_df.printSchema()\n",
    "\n",
    "test_df.show(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1a4bba2e-d701-48fe-bd32-6d17be44e77e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import argparse\n",
    "# def parse_args():\n",
    "#     \"\"\"Parse command line arguments.\"\"\"\n",
    "#     home_dir = HOME\n",
    "\n",
    "#     parser = argparse.ArgumentParser(\n",
    "#         description=\"Process test image dataset stored on PACE.\"\n",
    "#     )\n",
    "#     parser.add_argument(\n",
    "#         \"--cores\",\n",
    "#         type=int,\n",
    "#         default=os.cpu_count(),\n",
    "#         help=\"Number of cores used in Spark driver\",\n",
    "#     )\n",
    "#     parser.add_argument(\n",
    "#         \"--memory\",\n",
    "#         type=str,\n",
    "#         default=\"16g\",\n",
    "#         help=\"Amount of memory to use in Spark driver\",\n",
    "#     )\n",
    "#     return parser.parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7e872ea7-048b-4768-8c5a-a2637b267e64",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/02/22 17:52:38 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n",
      "25/02/22 17:52:43 WARN TaskSetManager: Stage 0 contains a task of very large size (8230 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# args = parse_args()\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "# Initialize Spark\n",
    "# Replace get_spark with your appropriate Spark initialization function if needed.\n",
    "cores = (os.cpu_count(),)\n",
    "memory = (os.environ.get(\"PYSPARK_DRIVER_MEMORY\", \"16g\"),)\n",
    "executor_memory = (os.environ.get(\"PYSPARK_EXECUTOR_MEMORY\", \"1g\"),)\n",
    "local_dir = (os.environ.get(\"SPARK_LOCAL_DIR\", \"/tmp\"),)\n",
    "app_name = \"clef\"\n",
    "local_dir = f\"{local_dir}/{int(time.time())}\"\n",
    "Path(local_dir).mkdir(parents=True, exist_ok=True)\n",
    "builder = (\n",
    "    SparkSession.builder.config(\"spark.driver.memory\", memory)\n",
    "    .config(\"spark.executor.memory\", executor_memory)\n",
    "    .config(\"spark.driver.cores\", cores)\n",
    "    .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\n",
    "    .config(\"spark.driver.maxResultSize\", \"8g\")\n",
    "    .config(\"spark.local.dir\", local_dir)\n",
    ")\n",
    "builder.appName(\"SegmentedImageStorage\").getOrCreate()\n",
    "df = spark.createDataFrame(records, schema=schema)\n",
    "image_final_df = df.select(\n",
    "    F.col(\"image_name\"), F.col(\"image\"), F.col(\"final_mask\"), F.col(\"class_masks\")\n",
    ").repartition(500)\n",
    "image_final_df.write.mode(\"overwrite\").parquet(\"segmented_images.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "86169f89-47b4-4a78-854f-1e092d2353ab",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/storage/home/hcoda1/2/dkhattak6/shared/plantclef\n"
     ]
    }
   ],
   "source": [
    "HOMEPATH = Path(os.path.expanduser(\"~/shared/plantclef\"))\n",
    "print(HOMEPATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e3a1ab8-a120-4e3c-81dd-dff2f5ecb11b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SAMKernel",
   "language": "python",
   "name": "samkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
