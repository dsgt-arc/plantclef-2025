{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a59db57",
   "metadata": {},
   "source": [
    "# Single-label image classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c0d95e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aad60cf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/04/16 13:40:09 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/04/16 13:40:09 WARN SparkConf: Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://atl1-1-02-005-31-0.pace.gatech.edu:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.5</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[4]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>clef</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fffbf590d00>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from plantclef.spark import get_spark\n",
    "\n",
    "spark = get_spark(cores=4)\n",
    "display(spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be6dba41",
   "metadata": {},
   "source": [
    "### embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c8074208",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Apr 16 01:40:11 PM EDT 2025\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Get list of stored filed in cloud bucket\n",
    "root = Path(os.path.expanduser(\"~\"))\n",
    "! date"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d37e9a77",
   "metadata": {},
   "source": [
    "### test embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "909f3b5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- image_name: string (nullable = true)\n",
      " |-- path: string (nullable = true)\n",
      " |-- data: binary (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1:>                                                          (0 + 1) / 1]\r"
     ]
    }
   ],
   "source": [
    "# Path and dataset names\n",
    "data_path = f\"{root}/p-dsgt_clef2025-0/shared/plantclef/data/parquet\"\n",
    "\n",
    "# Define the path to the parquet files\n",
    "test_path = f\"{data_path}/test_2025\"\n",
    "\n",
    "# Read the parquet files into a spark DataFrame\n",
    "test_df = spark.read.parquet(test_path)\n",
    "\n",
    "# Show the data\n",
    "test_df.printSchema()\n",
    "test_df.show(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c079f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import timm\n",
    "import torch\n",
    "from plantclef.model_setup import setup_fine_tuned_model\n",
    "\n",
    "\n",
    "num_classes = 7806  # total number of plant species\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = timm.create_model(\n",
    "    \"vit_base_patch14_reg4_dinov2.lvd142m\",\n",
    "    pretrained=False,\n",
    "    num_classes=num_classes,\n",
    "    checkpoint_path=setup_fine_tuned_model(),\n",
    ")\n",
    "# Data transform\n",
    "data_config = timm.data.resolve_model_data_config(model)\n",
    "transforms = timm.data.create_transform(**data_config, is_training=False)\n",
    "# Move model to GPU if available\n",
    "model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f800394f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "# load the cluster probabilities dataframes\n",
    "def get_cluster_probability_dfs():\n",
    "    clustering_path = \"~/p-dsgt_clef2025-0/shared/plantclef/data/clustering\"\n",
    "    test_cluster_csv = f\"{clustering_path}/test_2025_dominant_clusters.csv\"\n",
    "    test_cluster_probabilities = (\n",
    "        f\"{clustering_path}/test_2025_embed_probabilities_clustered\"\n",
    "    )\n",
    "    cluster_df = pd.read_csv(test_cluster_csv)\n",
    "    probabilities_df = pd.read_parquet(test_cluster_probabilities)\n",
    "    return cluster_df, probabilities_df\n",
    "\n",
    "\n",
    "def get_prior_for_image(image_name, cluster_df, probabilities_df) -> dict:\n",
    "    row = cluster_df[cluster_df[\"image_name\"] == image_name]\n",
    "    cluster_id = row.iloc[0][\"kmeans_cluster\"]\n",
    "    prior_row = probabilities_df[probabilities_df[\"dominant_cluster\"] == cluster_id]\n",
    "    return prior_row.iloc[0][\"renormalized_probabilities\"]\n",
    "\n",
    "\n",
    "cluster_df, probabilities_df = get_cluster_probability_dfs()\n",
    "display(cluster_df.head(5))\n",
    "display(probabilities_df.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76512ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from plantclef.config import get_class_mappings_file\n",
    "\n",
    "use_grid = True\n",
    "use_prior = True\n",
    "\n",
    "\n",
    "def load_class_mapping(class_mapping_file):\n",
    "    with open(class_mapping_file) as f:\n",
    "        class_index_to_class_name = {i: line.strip() for i, line in enumerate(f)}\n",
    "    return class_index_to_class_name\n",
    "\n",
    "\n",
    "def split_into_grid(image, grid_size=4):\n",
    "    w, h = image.size\n",
    "    grid_w, grid_h = w // grid_size, h // grid_size\n",
    "    images = []\n",
    "    for i in range(grid_size):\n",
    "        for j in range(grid_size):\n",
    "            left = i * grid_w\n",
    "            upper = j * grid_h\n",
    "            right = left + grid_w\n",
    "            lower = upper + grid_h\n",
    "            crop_image = image.crop((left, upper, right, lower))\n",
    "            images.append(crop_image)\n",
    "    return images\n",
    "\n",
    "\n",
    "class_mapping_file = get_class_mappings_file()\n",
    "cid_to_spid = load_class_mapping(class_mapping_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3efcdf46",
   "metadata": {},
   "outputs": [],
   "source": [
    "from plantclef.serde import deserialize_image\n",
    "\n",
    "\n",
    "# predict single image\n",
    "def predict(input_data, image_name):\n",
    "    img = deserialize_image(input_data)  # from bytes to PIL image\n",
    "    top_k_proba = 10\n",
    "    limit_logits = 10\n",
    "    images = [img]\n",
    "    # use grid to get logits\n",
    "    if use_grid:\n",
    "        images = split_into_grid(img)\n",
    "    results = []\n",
    "    for tile in images:\n",
    "        processed_image = transforms(tile).unsqueeze(0).to(device)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(processed_image)\n",
    "            probabilities = torch.softmax(outputs, dim=1) * 100\n",
    "            if use_prior:\n",
    "                prior = get_prior_for_image(image_name)\n",
    "                probabilities = probabilities * torch.tensor(prior).to(device)\n",
    "            top_probs, top_indices = torch.topk(probabilities, k=top_k_proba)\n",
    "        top_probs = top_probs.cpu().numpy()[0]\n",
    "        top_indices = top_indices.cpu().numpy()[0]\n",
    "\n",
    "        result = [\n",
    "            {cid_to_spid[index]: float(prob)}\n",
    "            for index, prob in zip(top_indices, top_probs)\n",
    "        ]\n",
    "        results.append(result)\n",
    "    # flatten the results from all grids, get top probabilities\n",
    "    flattened_results = [item for grid in results for item in grid[:limit_logits]]\n",
    "    # sort by score in descending order\n",
    "    sorted_logits = sorted(flattened_results, key=lambda x: -list(x.values())[0])\n",
    "    return sorted_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b082e042",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select first image\n",
    "image_name = test_df.select(\"image_name\").first()[0]\n",
    "image_data = test_df.filter(test_df.image_name == image_name).first()\n",
    "type(image_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c294bc1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
