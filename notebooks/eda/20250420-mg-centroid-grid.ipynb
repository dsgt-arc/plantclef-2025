{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "125c1782",
   "metadata": {},
   "source": [
    "# Centroid grid predictions\n",
    "\n",
    "Classify test data using centroid probabilities for grid of tiles.\n",
    "We're using probabilities for the entire test image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bbf289d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1f4e03f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/04/20 15:54:48 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/04/20 15:54:49 WARN SparkConf: Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).\n",
      "25/04/20 15:54:49 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://atl1-1-02-002-29-0.pace.gatech.edu:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.5</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[4]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>clef</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fffbf59cd00>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from plantclef.spark import get_spark\n",
    "\n",
    "spark = get_spark(cores=4)\n",
    "display(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "659a785b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Apr 20 03:54:51 PM EDT 2025\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Get list of stored filed in cloud bucket\n",
    "root = Path(os.path.expanduser(\"~\"))\n",
    "! date"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23df9c9b",
   "metadata": {},
   "source": [
    "### Faiss centroid probabilities "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "86f9146c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- centroid_id: integer (nullable = true)\n",
      " |-- species_id: integer (nullable = true)\n",
      " |-- embedding: array (nullable = true)\n",
      " |    |-- element: float (containsNull = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+--------------------+\n",
      "|centroid_id|species_id|           embedding|\n",
      "+-----------+----------+--------------------+\n",
      "|          6|   1398243|[1.0948665, -0.35...|\n",
      "|          7|   1647175|[-0.30202305, 0.7...|\n",
      "|          3|   1360020|[0.049416766, 0.9...|\n",
      "|          3|   1361527|[0.3046188, 0.885...|\n",
      "|          8|   1359277|[-0.09973065, 0.6...|\n",
      "+-----------+----------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Path and dataset names\n",
    "data_path = f\"{root}/p-dsgt_clef2025-0/shared/plantclef/data/embeddings\"\n",
    "\n",
    "\n",
    "# Define the path to the train and test parquet files\n",
    "def get_faiss_embed_path(num_centroids: int = 10):\n",
    "    return f\"{data_path}/train_centroids/num_centroids={num_centroids}\"\n",
    "\n",
    "\n",
    "# Read the parquet files into a spark DataFrame\n",
    "faiss10_df = spark.read.parquet(get_faiss_embed_path(10))\n",
    "faiss20_df = spark.read.parquet(get_faiss_embed_path(20))\n",
    "faiss50_df = spark.read.parquet(get_faiss_embed_path(50))\n",
    "\n",
    "# Show the data\n",
    "faiss10_df.printSchema()\n",
    "faiss10_df.show(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8cff7bcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- image_name: string (nullable = true)\n",
      " |-- output: struct (nullable = true)\n",
      " |    |-- cls_token: array (nullable = true)\n",
      " |    |    |-- element: float (containsNull = true)\n",
      " |    |-- logits: array (nullable = true)\n",
      " |    |    |-- element: float (containsNull = true)\n",
      " |-- sample_id: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_path = f\"{data_path}/test_2025/test_2025_embed_logits\"\n",
    "test_df = spark.read.parquet(test_path)\n",
    "test_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0a8fd923",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- species_id: integer (nullable = true)\n",
      " |-- embedding_list: array (nullable = false)\n",
      " |    |-- element: array (containsNull = false)\n",
      " |    |    |-- element: float (containsNull = true)\n",
      " |-- avg_embeddings: array (nullable = true)\n",
      " |    |-- element: float (containsNull = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 8:===============================================>           (4 + 1) / 5]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------------------------------------+--------------------------------------------------+\n",
      "|species_id|                                    embedding_list|                                    avg_embeddings|\n",
      "+----------+--------------------------------------------------+--------------------------------------------------+\n",
      "|   1355869|[[0.0038829704, -0.013802218, 0.009791469, 0.36...|[0.114047855, 0.4057617, -0.06062866, 0.0549097...|\n",
      "|   1355870|[[0.18335515, -0.08915622, -0.29222322, 0.81695...|[0.11070547, 0.28042978, -0.69385886, 0.3224701...|\n",
      "|   1355872|[[0.28893483, 0.20109488, 0.24363865, 0.2530085...|[0.19194208, 0.1617909, -0.013561882, 0.1457786...|\n",
      "|   1355881|[[0.23531002, -0.26157096, -0.017927974, -1.038...|[0.20774464, -0.2938113, 0.026279427, -0.838810...|\n",
      "|   1355900|[[1.0873219, -0.07327607, -0.29321098, 0.071418...|[0.5382732, 0.15868045, -0.116654, 0.23893037, ...|\n",
      "|   1355953|[[-0.4760781, 1.7744097, -1.1470047, -0.3837139...|[-0.016290855, 0.59006137, -0.39261013, -0.5602...|\n",
      "|   1355964|[[0.48310524, 0.46739435, 0.27038592, -0.102291...|[0.60323864, -0.079119764, 0.027320625, 0.21178...|\n",
      "|   1355969|[[-0.12063713, -0.28975615, 0.66797143, 0.35018...|[0.4102314, -0.033057135, -0.3156675, 0.1761870...|\n",
      "|   1355972|[[0.43435532, 0.7576206, -0.27943552, 0.0484088...|[0.32022467, 0.7524549, -0.4475748, 0.1758367, ...|\n",
      "|   1355987|[[0.65558386, -0.5485311, -0.1604925, 0.2336161...|[0.46767733, -0.31536704, -0.33670023, -0.15982...|\n",
      "+----------+--------------------------------------------------+--------------------------------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import ArrayType, FloatType\n",
    "\n",
    "\n",
    "def avg_embeddings_udf(embeddings):\n",
    "    array = np.array(embeddings)\n",
    "    mean_array = np.mean(array, axis=0)\n",
    "    return mean_array.tolist()\n",
    "\n",
    "\n",
    "average_embeddings = F.udf(avg_embeddings_udf, ArrayType(FloatType()))\n",
    "\n",
    "# group and apply the UDF\n",
    "avg_embeddings_df = (\n",
    "    faiss10_df.groupBy(\"species_id\")\n",
    "    .agg(F.collect_list(\"embedding\").alias(\"embedding_list\"))\n",
    "    .withColumn(\"avg_embeddings\", average_embeddings(F.col(\"embedding_list\")))\n",
    ")\n",
    "avg_embeddings_df.printSchema()\n",
    "avg_embeddings_df.show(n=10, truncate=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "944829f8",
   "metadata": {},
   "source": [
    "### classifier-based probabilities\n",
    "\n",
    "Calculate probabilities based on embedding distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "325453a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(7806, 768)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics.pairwise import cosine_similarity, euclidean_distances\n",
    "from scipy.special import softmax\n",
    "from plantclef.config import get_class_mappings_file\n",
    "\n",
    "# load class mappings\n",
    "class_mappings_file = get_class_mappings_file()\n",
    "with open(class_mappings_file) as f:\n",
    "    sorted_species_ids = [int(line.strip()) for line in f]\n",
    "\n",
    "# get (species_id, avg_embeddings) from Spark\n",
    "centroids_pd = avg_embeddings_df.select(\"species_id\", \"avg_embeddings\").toPandas()\n",
    "\n",
    "# filter + reorder centroids to match sorted_species_ids\n",
    "centroids_dict = dict(zip(centroids_pd[\"species_id\"], centroids_pd[\"avg_embeddings\"]))\n",
    "filtered_embeddings = [\n",
    "    centroids_dict[species_id]\n",
    "    if species_id in centroids_dict\n",
    "    else np.zeros_like(next(iter(centroids_dict.values())))\n",
    "    for species_id in sorted_species_ids\n",
    "]\n",
    "\n",
    "# shape: (num_species, embedding_dim)\n",
    "train_embeddings = np.stack(filtered_embeddings)\n",
    "train_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ce6745",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get test embeddings and image names\n",
    "test_pd = test_df.select(\"image_name\", \"output.cls_token\").toPandas()\n",
    "test_embeddings = np.stack(test_pd[\"cls_token\"].values)\n",
    "image_names = test_pd[\"image_name\"].values\n",
    "\n",
    "# compute cosine similarity and softmax\n",
    "cos_similarities = cosine_similarity(test_embeddings, train_embeddings)\n",
    "eucliden_dist = euclidean_distances(test_embeddings, train_embeddings)\n",
    "euclidean_score = (1 / (eucliden_dist**2)) / np.sum(\n",
    "    1 / (eucliden_dist**2), axis=1, keepdims=True\n",
    ")  # normalize to sum to 1\n",
    "cos_probabilities = softmax(cos_similarities, axis=1)  # shape: (num_test, num_species)\n",
    "\n",
    "# create final DataFrame with aligned probabilities\n",
    "final_df = pd.DataFrame(\n",
    "    {\n",
    "        \"image_name\": image_names,\n",
    "        \"cos_probabilities\": list(cos_probabilities),\n",
    "        \"euclidean_score\": list(euclidean_score),\n",
    "    }\n",
    ")\n",
    "final_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2046ed5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c1e4fb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
