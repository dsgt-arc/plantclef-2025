{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "125c1782",
   "metadata": {},
   "source": [
    "# Centroid grid predictions\n",
    "\n",
    "Classify test data using centroid probabilities for grid of tiles.\n",
    "We're using probabilities for the entire test image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bbf289d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1f4e03f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/04/20 17:12:54 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/04/20 17:12:54 WARN SparkConf: Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).\n",
      "25/04/20 17:12:54 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://atl1-1-02-002-29-0.pace.gatech.edu:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.5</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[4]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>clef</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fffbf590d00>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from plantclef.spark import get_spark\n",
    "\n",
    "spark = get_spark(cores=4)\n",
    "display(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "659a785b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Apr 20 05:12:56 PM EDT 2025\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Get list of stored filed in cloud bucket\n",
    "root = Path(os.path.expanduser(\"~\"))\n",
    "! date"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23df9c9b",
   "metadata": {},
   "source": [
    "### Faiss centroid probabilities "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "86f9146c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- centroid_id: integer (nullable = true)\n",
      " |-- species_id: integer (nullable = true)\n",
      " |-- embedding: array (nullable = true)\n",
      " |    |-- element: float (containsNull = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+--------------------+\n",
      "|centroid_id|species_id|           embedding|\n",
      "+-----------+----------+--------------------+\n",
      "|          6|   1398243|[1.0948665, -0.35...|\n",
      "|          7|   1647175|[-0.30202305, 0.7...|\n",
      "|          3|   1360020|[0.049416766, 0.9...|\n",
      "|          3|   1361527|[0.3046188, 0.885...|\n",
      "|          8|   1359277|[-0.09973065, 0.6...|\n",
      "+-----------+----------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Path and dataset names\n",
    "data_path = f\"{root}/p-dsgt_clef2025-0/shared/plantclef/data/embeddings\"\n",
    "\n",
    "\n",
    "# Define the path to the train and test parquet files\n",
    "def get_faiss_embed_path(num_centroids: int = 10):\n",
    "    return f\"{data_path}/train_centroids/num_centroids={num_centroids}\"\n",
    "\n",
    "\n",
    "# Read the parquet files into a spark DataFrame\n",
    "faiss10_df = spark.read.parquet(get_faiss_embed_path(10))\n",
    "faiss20_df = spark.read.parquet(get_faiss_embed_path(20))\n",
    "faiss50_df = spark.read.parquet(get_faiss_embed_path(50))\n",
    "\n",
    "# Show the data\n",
    "faiss10_df.printSchema()\n",
    "faiss10_df.show(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8cff7bcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- image_name: string (nullable = true)\n",
      " |-- output: struct (nullable = true)\n",
      " |    |-- cls_token: array (nullable = true)\n",
      " |    |    |-- element: float (containsNull = true)\n",
      " |    |-- logits: array (nullable = true)\n",
      " |    |    |-- element: float (containsNull = true)\n",
      " |-- sample_id: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_path = f\"{data_path}/test_2025/test_2025_embed_logits\"\n",
    "test_df = spark.read.parquet(test_path)\n",
    "test_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1c9a87bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- image_name: string (nullable = true)\n",
      " |-- path: string (nullable = true)\n",
      " |-- data: binary (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Path and dataset names\n",
    "data_path = f\"{root}/p-dsgt_clef2025-0/shared/plantclef/data\"\n",
    "test_parquet_path = f\"{data_path}/parquet/test_2025\"\n",
    "df = spark.read.parquet(test_parquet_path)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c99f412",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- image_name: string (nullable = true)\n",
      " |-- tile_index: integer (nullable = true)\n",
      " |-- tile: binary (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+--------------------+\n",
      "|          image_name|tile_index|                tile|\n",
      "+--------------------+----------+--------------------+\n",
      "|CBN-Pla-B3-201907...|         0|[89 50 4E 47 0D 0...|\n",
      "|CBN-Pla-B3-201907...|         1|[89 50 4E 47 0D 0...|\n",
      "|CBN-Pla-B3-201907...|         2|[89 50 4E 47 0D 0...|\n",
      "|CBN-Pla-B3-201907...|         3|[89 50 4E 47 0D 0...|\n",
      "|CBN-Pla-B3-201907...|         4|[89 50 4E 47 0D 0...|\n",
      "|CBN-Pla-B3-201907...|         5|[89 50 4E 47 0D 0...|\n",
      "|CBN-Pla-B3-201907...|         6|[89 50 4E 47 0D 0...|\n",
      "|CBN-Pla-B3-201907...|         7|[89 50 4E 47 0D 0...|\n",
      "|CBN-Pla-B3-201907...|         8|[89 50 4E 47 0D 0...|\n",
      "|CBN-Pla-B3-201907...|         9|[89 50 4E 47 0D 0...|\n",
      "|CBN-Pla-B3-201907...|        10|[89 50 4E 47 0D 0...|\n",
      "|CBN-Pla-B3-201907...|        11|[89 50 4E 47 0D 0...|\n",
      "|CBN-Pla-B3-201907...|        12|[89 50 4E 47 0D 0...|\n",
      "|CBN-Pla-B3-201907...|        13|[89 50 4E 47 0D 0...|\n",
      "|CBN-Pla-B3-201907...|        14|[89 50 4E 47 0D 0...|\n",
      "|CBN-Pla-B3-201907...|        15|[89 50 4E 47 0D 0...|\n",
      "+--------------------+----------+--------------------+\n",
      "only showing top 16 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from plantclef.serde import deserialize_image, serialize_image\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import (\n",
    "    BinaryType,\n",
    "    ArrayType,\n",
    "    StructType,\n",
    "    StructField,\n",
    "    IntegerType,\n",
    ")\n",
    "\n",
    "\n",
    "def split_into_grid(image: Image.Image, grid_size: int = 4):\n",
    "    w, h = image.size\n",
    "    grid_w, grid_h = w // grid_size, h // grid_size\n",
    "    tiles = []\n",
    "    for i in range(grid_size):\n",
    "        for j in range(grid_size):\n",
    "            left = i * grid_w\n",
    "            upper = j * grid_h\n",
    "            right = left + grid_w\n",
    "            lower = upper + grid_h\n",
    "            tile = image.crop((left, upper, right, lower))\n",
    "            tiles.append(tile)\n",
    "    return tiles\n",
    "\n",
    "\n",
    "@F.pandas_udf(\n",
    "    ArrayType(\n",
    "        StructType(\n",
    "            [\n",
    "                StructField(\"tile_index\", IntegerType()),\n",
    "                StructField(\"tile\", BinaryType()),\n",
    "            ]\n",
    "        )\n",
    "    )\n",
    ")\n",
    "def get_image_tiles(data_series: pd.Series, grid_size: int = 4) -> pd.Series:\n",
    "    all_tiles = []\n",
    "    for img_byte in data_series:\n",
    "        img = deserialize_image(img_byte)\n",
    "        tiles = split_into_grid(img, grid_size=grid_size)\n",
    "        tile_structs = [\n",
    "            {\"tile_index\": i, \"tile\": serialize_image(tile)}\n",
    "            for i, tile in enumerate(tiles)\n",
    "        ]\n",
    "        all_tiles.append(tile_structs)\n",
    "    return pd.Series(all_tiles)\n",
    "\n",
    "\n",
    "df_with_tiles = df.withColumn(\"tiles\", get_image_tiles(\"data\", grid_size=4))\n",
    "df_tiles = df_with_tiles.select(\"image_name\", F.explode(\"tiles\").alias(\"tile_struct\"))\n",
    "df_tiles = df_tiles.select(\n",
    "    \"image_name\",\n",
    "    F.col(\"tile_struct.tile_index\").alias(\"tile_index\"),\n",
    "    F.col(\"tile_struct.tile\").alias(\"tile\"),\n",
    ")\n",
    "df_tiles.printSchema()\n",
    "df_tiles.show(n=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a8fd923",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import ArrayType, FloatType\n",
    "\n",
    "\n",
    "def avg_embeddings_udf(embeddings):\n",
    "    array = np.array(embeddings)\n",
    "    mean_array = np.mean(array, axis=0)\n",
    "    return mean_array.tolist()\n",
    "\n",
    "\n",
    "average_embeddings = F.udf(avg_embeddings_udf, ArrayType(FloatType()))\n",
    "\n",
    "# group and apply the UDF\n",
    "avg_embeddings_df = (\n",
    "    faiss10_df.groupBy(\"species_id\")\n",
    "    .agg(F.collect_list(\"embedding\").alias(\"embedding_list\"))\n",
    "    .withColumn(\"avg_embeddings\", average_embeddings(F.col(\"embedding_list\")))\n",
    ")\n",
    "avg_embeddings_df.printSchema()\n",
    "avg_embeddings_df.show(n=10, truncate=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "944829f8",
   "metadata": {},
   "source": [
    "### classifier-based probabilities\n",
    "\n",
    "Calculate probabilities based on embedding distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "325453a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics.pairwise import cosine_similarity, euclidean_distances\n",
    "from scipy.special import softmax\n",
    "from plantclef.config import get_class_mappings_file\n",
    "\n",
    "# load class mappings\n",
    "class_mappings_file = get_class_mappings_file()\n",
    "with open(class_mappings_file) as f:\n",
    "    sorted_species_ids = [int(line.strip()) for line in f]\n",
    "\n",
    "# get (species_id, avg_embeddings) from Spark\n",
    "centroids_pd = avg_embeddings_df.select(\"species_id\", \"avg_embeddings\").toPandas()\n",
    "\n",
    "# filter + reorder centroids to match sorted_species_ids\n",
    "centroids_dict = dict(zip(centroids_pd[\"species_id\"], centroids_pd[\"avg_embeddings\"]))\n",
    "filtered_embeddings = [\n",
    "    centroids_dict[species_id]\n",
    "    if species_id in centroids_dict\n",
    "    else np.zeros_like(next(iter(centroids_dict.values())))\n",
    "    for species_id in sorted_species_ids\n",
    "]\n",
    "\n",
    "# shape: (num_species, embedding_dim)\n",
    "train_embeddings = np.stack(filtered_embeddings)\n",
    "train_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ce6745",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get test embeddings and image names\n",
    "test_pd = test_df.select(\"image_name\", \"output.cls_token\").toPandas()\n",
    "test_embeddings = np.stack(test_pd[\"cls_token\"].values)\n",
    "image_names = test_pd[\"image_name\"].values\n",
    "\n",
    "# compute cosine similarity and softmax\n",
    "cos_similarities = cosine_similarity(test_embeddings, train_embeddings)\n",
    "eucliden_dist = euclidean_distances(test_embeddings, train_embeddings)\n",
    "euclidean_score = (1 / (eucliden_dist**2)) / np.sum(\n",
    "    1 / (eucliden_dist**2), axis=1, keepdims=True\n",
    ")  # normalize to sum to 1\n",
    "cos_probabilities = softmax(cos_similarities, axis=1)  # shape: (num_test, num_species)\n",
    "\n",
    "# create final DataFrame with aligned probabilities\n",
    "final_df = pd.DataFrame(\n",
    "    {\n",
    "        \"image_name\": image_names,\n",
    "        \"cos_probabilities\": list(cos_probabilities),\n",
    "        \"euclidean_score\": list(euclidean_score),\n",
    "    }\n",
    ")\n",
    "final_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2046ed5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c1e4fb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
