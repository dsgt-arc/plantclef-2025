{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "baf99764",
   "metadata": {},
   "source": [
    "# Species Prior\n",
    "PaCMAP repo: https://github.com/YingfanWang/PaCMAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a58405da",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fb3662d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/04/16 11:16:45 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/04/16 11:16:45 WARN SparkConf: Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://atl1-1-02-005-31-0.pace.gatech.edu:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.5</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[4]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>clef</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fffbf59cd00>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from plantclef.spark import get_spark\n",
    "\n",
    "spark = get_spark(cores=4)\n",
    "display(spark)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2fb9399",
   "metadata": {},
   "source": [
    "### embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "64115e67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Apr 16 11:16:48 AM EDT 2025\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Get list of stored filed in cloud bucket\n",
    "root = Path(os.path.expanduser(\"~\"))\n",
    "! date"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53a70541",
   "metadata": {},
   "source": [
    "### test embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f60c0043",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- image_name: string (nullable = true)\n",
      " |-- output: struct (nullable = true)\n",
      " |    |-- cls_token: array (nullable = true)\n",
      " |    |    |-- element: float (containsNull = true)\n",
      " |    |-- logits: array (nullable = true)\n",
      " |    |    |-- element: float (containsNull = true)\n",
      " |-- sample_id: integer (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+---------+\n",
      "|          image_name|              output|sample_id|\n",
      "+--------------------+--------------------+---------+\n",
      "|CBN-Pla-A1-201908...|{[0.47354543, 1.5...|        0|\n",
      "|CBN-Pla-D6-201908...|{[-0.39621377, 1....|        0|\n",
      "|CBN-PdlC-C5-20140...|{[-0.5331654, 0.2...|        0|\n",
      "|LISAH-BOU-0-37-20...|{[1.2480925, 0.47...|        0|\n",
      "|CBN-Pla-E4-201308...|{[0.7065191, 1.70...|        0|\n",
      "+--------------------+--------------------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Path and dataset names\n",
    "data_path = f\"{root}/p-dsgt_clef2025-0/shared/plantclef/data/embeddings\"\n",
    "\n",
    "# Define the path to the train and test parquet files\n",
    "test_path = f\"{data_path}/test_2025/test_2025_embed_logits\"\n",
    "\n",
    "# Read the parquet files into a spark DataFrame\n",
    "test_df = spark.read.parquet(test_path)\n",
    "\n",
    "# Show the data\n",
    "test_df.printSchema()\n",
    "test_df.show(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "05c2c024",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows: 2105\n"
     ]
    }
   ],
   "source": [
    "# count number of rows: should be 2105 for grid=1x1\n",
    "print(f\"Number of rows: {test_df.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "65967316",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regions = [\n",
    "    \"2024-CEV3\",\n",
    "    \"CBN-can\",\n",
    "    \"CBN-PdlC\",\n",
    "    \"CBN-Pla\",\n",
    "    \"CBN-Pyr\",\n",
    "    \"GUARDEN-AMB\",\n",
    "    \"GUARDEN-CBNMed\",\n",
    "    \"LISAH-BOU\",\n",
    "    \"LISAH-BVD\",\n",
    "    \"LISAH-JAS\",\n",
    "    \"LISAH-PEC\",\n",
    "    \"OPTMix\",\n",
    "    \"RNNB\",\n",
    "]\n",
    "\n",
    "len(regions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3f8737d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows with unmatched regions: 0\n",
      "+--------------------+---------------+--------------------+\n",
      "|          image_name|grouped_regions|           cls_token|\n",
      "+--------------------+---------------+--------------------+\n",
      "|CBN-Pla-A1-201908...|        CBN-Pla|[0.47354543, 1.55...|\n",
      "|CBN-Pla-D6-201908...|        CBN-Pla|[-0.39621377, 1.2...|\n",
      "|CBN-PdlC-C5-20140...|       CBN-PdlC|[-0.5331654, 0.21...|\n",
      "|LISAH-BOU-0-37-20...|      LISAH-BOU|[1.2480925, 0.478...|\n",
      "|CBN-Pla-E4-201308...|        CBN-Pla|[0.7065191, 1.709...|\n",
      "|CBN-PdlC-D6-20150...|       CBN-PdlC|[-0.32394692, 0.4...|\n",
      "|CBN-PdlC-F2-20170...|       CBN-PdlC|[1.4019761, 1.783...|\n",
      "|CBN-PdlC-A6-20180...|       CBN-PdlC|[-0.49399343, 1.1...|\n",
      "|RNNB-3-12-2023051...|           RNNB|[-0.37940657, 0.1...|\n",
      "|CBN-PdlC-F4-20150...|       CBN-PdlC|[-0.26687536, 1.2...|\n",
      "+--------------------+---------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "+---------------+-----+\n",
      "|grouped_regions|count|\n",
      "+---------------+-----+\n",
      "|       CBN-PdlC|  816|\n",
      "|        CBN-Pla|  628|\n",
      "| GUARDEN-CBNMed|  165|\n",
      "|           RNNB|  141|\n",
      "|      LISAH-BOU|   82|\n",
      "|         OPTMix|   78|\n",
      "|      LISAH-BVD|   76|\n",
      "|    GUARDEN-AMB|   36|\n",
      "|      LISAH-PEC|   35|\n",
      "|        CBN-can|   30|\n",
      "|      LISAH-JAS|   15|\n",
      "|        CBN-Pyr|    2|\n",
      "|      2024-CEV3|    1|\n",
      "+---------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.functions import col, regexp_extract\n",
    "\n",
    "\n",
    "def prepare_emb_df(df: DataFrame, embed_col: str = \"output.cls_token\") -> DataFrame:\n",
    "    \"\"\"\n",
    "    Prepare the DataFrame by renaming the embedding column and selecting relevant columns.\n",
    "    \"\"\"\n",
    "    regex_pattern = \"|\".join([f\"^{region}\" for region in regions])\n",
    "    # add the grouped_regions column using regexp_extract\n",
    "    test_df_with_regions = df.withColumn(\n",
    "        \"grouped_regions\", regexp_extract(col(\"image_name\"), f\"({regex_pattern})\", 1)\n",
    "    )\n",
    "\n",
    "    # check if there are any unmatched regions (empty strings)\n",
    "    unmatched_count = test_df_with_regions.filter(col(\"grouped_regions\") == \"\").count()\n",
    "    print(f\"Number of rows with unmatched regions: {unmatched_count}\")\n",
    "\n",
    "    # show the result with the new column\n",
    "    test_df_with_regions.select(\"image_name\", \"grouped_regions\", embed_col).show(\n",
    "        10, truncate=True\n",
    "    )\n",
    "\n",
    "    # Count occurrences of each region\n",
    "    region_counts = (\n",
    "        test_df_with_regions.groupBy(\"grouped_regions\")\n",
    "        .count()\n",
    "        .orderBy(col(\"count\").desc())\n",
    "    )\n",
    "    region_counts.show(20)\n",
    "    return test_df_with_regions\n",
    "\n",
    "\n",
    "test_df_with_regions = prepare_emb_df(test_df, \"output.cls_token\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4ddcb912",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2105"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df_with_regions.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ff790870",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pacmap\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "\n",
    "class PaCMAPTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, n_components=2, random_state=42):\n",
    "        self.n_components = n_components\n",
    "        self.random_state = random_state\n",
    "        self.reducer = None\n",
    "        self.embedding_ = None\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        self.reducer = pacmap.PaCMAP(\n",
    "            n_components=self.n_components, random_state=self.random_state\n",
    "        )\n",
    "        self.embedding_ = self.reducer.fit_transform(X)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        # just return the already-computed embedding\n",
    "        return self.embedding_\n",
    "\n",
    "\n",
    "clustering_pipeline = Pipeline(\n",
    "    [\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"pacmap\", PaCMAPTransformer(n_components=2, random_state=42)),\n",
    "        (\"cluster\", KMeans(n_clusters=5, random_state=42)),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ef755fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# model directory\n",
    "model_dir = (\n",
    "    Path(os.path.expanduser(\"~\")) / \"p-dsgt_clef2025-0/shared/plantclef/models/pacmap\"\n",
    ")\n",
    "pipeline_filename = model_dir / \"plant_clustering_pipeline.pkl\"\n",
    "\n",
    "\n",
    "# load the clustering model\n",
    "with open(pipeline_filename, \"rb\") as file:\n",
    "    clustering_pipeline = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5698aa40",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:pacmap.pacmap:Warning: random state is set to 42.                       \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Convert to Pandas DF\n",
    "col_name = \"output.cls_token\"\n",
    "df = test_df_with_regions.select([col_name, \"grouped_regions\"])\n",
    "pandas_df = df.select([\"cls_token\", \"grouped_regions\"]).toPandas()\n",
    "\n",
    "# Fit and predict clusters\n",
    "embeddings = np.stack(pandas_df[\"cls_token\"].values)\n",
    "labels = clustering_pipeline.fit_predict(embeddings)\n",
    "\n",
    "# Attach results to the DataFrame\n",
    "pandas_df[\"cluster\"] = labels\n",
    "pandas_df[\"pacmap_1\"] = clustering_pipeline.named_steps[\"pacmap\"].embedding_[:, 0]\n",
    "pandas_df[\"pacmap_2\"] = clustering_pipeline.named_steps[\"pacmap\"].embedding_[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a63cd88f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 2, 1, 4, 2, 3, 1, 3, 0, 1], dtype=int32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels[:10]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5077947b",
   "metadata": {},
   "source": [
    "### TODO\n",
    "\n",
    "**Question:** what species belong to each cluster?\n",
    "\n",
    "1. get the entire classification logits for each test image\n",
    "2. average the probabilities (softmax the logits) for each cluster\n",
    "3. get the most probable species for each cluster --> get the most probable species for each location\n",
    "4. group them by genus and family and do the same for steps 3 & 4\n",
    "\n",
    "Can we narrow down genus ~100 per cluster?\n",
    "\n",
    "**Main goal:** select a subset of candidate species to choose from when making predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f3725852",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 16:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------+--------------------------------------------------+---------+---------------+----------------+\n",
      "|                 image_name|                                            output|sample_id|grouped_regions|dominant_cluster|\n",
      "+---------------------------+--------------------------------------------------+---------+---------------+----------------+\n",
      "|    CBN-Pla-A1-20190814.jpg|{[0.47354543, 1.5568701, -1.6330245, -1.3648611...|        0|        CBN-Pla|               2|\n",
      "|    CBN-Pla-D6-20190814.jpg|{[-0.39621377, 1.2026826, 0.27647698, -0.661421...|        0|        CBN-Pla|               2|\n",
      "|   CBN-PdlC-C5-20140901.jpg|{[-0.5331654, 0.21328913, -1.2809799, 0.1238243...|        0|       CBN-PdlC|               1|\n",
      "|LISAH-BOU-0-37-20230512.jpg|{[1.2480925, 0.4781976, 0.69301766, 0.4653994, ...|        0|      LISAH-BOU|               0|\n",
      "|    CBN-Pla-E4-20130808.jpg|{[0.7065191, 1.7097996, -1.2477401, 1.3419615, ...|        0|        CBN-Pla|               2|\n",
      "|   CBN-PdlC-D6-20150701.jpg|{[-0.32394692, 0.48853773, -0.7659284, 1.658636...|        0|       CBN-PdlC|               1|\n",
      "|   CBN-PdlC-F2-20170906.jpg|{[1.4019761, 1.783083, -2.1123226, 0.29953066, ...|        0|       CBN-PdlC|               1|\n",
      "|   CBN-PdlC-A6-20180905.jpg|{[-0.49399343, 1.13436, 0.76335686, 0.32270005,...|        0|       CBN-PdlC|               1|\n",
      "|     RNNB-3-12-20230512.jpg|{[-0.37940657, 0.11167338, 0.65568197, 2.197065...|        0|           RNNB|               0|\n",
      "|   CBN-PdlC-F4-20150810.jpg|{[-0.26687536, 1.2176274, -3.113984, 0.40082005...|        0|       CBN-PdlC|               1|\n",
      "+---------------------------+--------------------------------------------------+---------+---------------+----------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import IntegerType\n",
    "\n",
    "# retrieved from clustering notebook\n",
    "dominant_clusters = {\n",
    "    \"GUARDEN-CBNMed\": 0,\n",
    "    \"RNNB\": 0,\n",
    "    \"LISAH-BOU\": 0,\n",
    "    \"OPTMix\": 0,\n",
    "    \"LISAH-BVD\": 0,\n",
    "    \"GUARDEN-AMB\": 0,\n",
    "    \"LISAH-PEC\": 0,\n",
    "    \"LISAH-JAS\": 0,\n",
    "    \"CBN-Pyr\": 0,\n",
    "    \"2024-CEV3\": 0,\n",
    "    \"CBN-PdlC\": 1,\n",
    "    \"CBN-can\": 1,\n",
    "    \"CBN-Pla\": 2,\n",
    "}\n",
    "\n",
    "\n",
    "# add dominant cluster to the test_df_with_regions based on grouped_regions\n",
    "def get_dominant_cluster(region):\n",
    "    return dominant_clusters.get(region, 0)  # Default to 0 if region not found\n",
    "\n",
    "\n",
    "get_dominant_cluster_udf = F.udf(get_dominant_cluster, IntegerType())\n",
    "cluster_df = test_df_with_regions.withColumn(\n",
    "    \"dominant_cluster\", get_dominant_cluster_udf(\"grouped_regions\")\n",
    ")\n",
    "# Show the updated DataFrame\n",
    "cluster_df.show(n=10, truncate=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "691e9cd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 17:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------+--------------------------------------------------+---------+---------------+----------------+--------------------------------------------------+\n",
      "|                 image_name|                                            output|sample_id|grouped_regions|dominant_cluster|                                     probabilities|\n",
      "+---------------------------+--------------------------------------------------+---------+---------------+----------------+--------------------------------------------------+\n",
      "|    CBN-Pla-A1-20190814.jpg|{[0.47354543, 1.5568701, -1.6330245, -1.3648611...|        0|        CBN-Pla|               2|[4.5976332E-5, 1.9088793E-5, 2.4028224E-5, 8.92...|\n",
      "|    CBN-Pla-D6-20190814.jpg|{[-0.39621377, 1.2026826, 0.27647698, -0.661421...|        0|        CBN-Pla|               2|[4.5638313E-5, 7.7455275E-5, 6.034234E-5, 1.287...|\n",
      "|   CBN-PdlC-C5-20140901.jpg|{[-0.5331654, 0.21328913, -1.2809799, 0.1238243...|        0|       CBN-PdlC|               1|[4.9514707E-5, 3.681495E-5, 2.595474E-5, 1.5290...|\n",
      "|LISAH-BOU-0-37-20230512.jpg|{[1.2480925, 0.4781976, 0.69301766, 0.4653994, ...|        0|      LISAH-BOU|               0|[1.5335014E-5, 1.0890696E-5, 1.909136E-5, 6.787...|\n",
      "|    CBN-Pla-E4-20130808.jpg|{[0.7065191, 1.7097996, -1.2477401, 1.3419615, ...|        0|        CBN-Pla|               2|[4.8469224E-6, 4.4648878E-6, 3.6098156E-6, 1.51...|\n",
      "|   CBN-PdlC-D6-20150701.jpg|{[-0.32394692, 0.48853773, -0.7659284, 1.658636...|        0|       CBN-PdlC|               1|[5.906881E-6, 6.386926E-5, 1.7545077E-5, 1.4986...|\n",
      "|   CBN-PdlC-F2-20170906.jpg|{[1.4019761, 1.783083, -2.1123226, 0.29953066, ...|        0|       CBN-PdlC|               1|[2.9472294E-5, 2.4534273E-5, 1.1312257E-4, 1.50...|\n",
      "|   CBN-PdlC-A6-20180905.jpg|{[-0.49399343, 1.13436, 0.76335686, 0.32270005,...|        0|       CBN-PdlC|               1|[1.6355605E-5, 1.3500267E-4, 3.7801985E-4, 8.49...|\n",
      "|     RNNB-3-12-20230512.jpg|{[-0.37940657, 0.11167338, 0.65568197, 2.197065...|        0|           RNNB|               0|[3.42797E-4, 2.1518086E-4, 4.9858834E-5, 1.2185...|\n",
      "|   CBN-PdlC-F4-20150810.jpg|{[-0.26687536, 1.2176274, -3.113984, 0.40082005...|        0|       CBN-PdlC|               1|[1.0782228E-6, 4.8797174E-6, 1.0285427E-5, 2.29...|\n",
      "+---------------------------+--------------------------------------------------+---------+---------------+----------------+--------------------------------------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from pyspark.sql.types import ArrayType, FloatType\n",
    "\n",
    "\n",
    "# get the probabilities from the output.logits column using softmax\n",
    "def get_probabilities(logits):\n",
    "    logits_tensor = torch.tensor(logits)\n",
    "    probabilities = torch.softmax(logits_tensor, dim=0)\n",
    "    return probabilities.tolist()\n",
    "\n",
    "\n",
    "get_probabilities_udf = F.udf(get_probabilities, ArrayType(FloatType()))\n",
    "cluster_df = cluster_df.withColumn(\n",
    "    \"probabilities\", get_probabilities_udf(\"output.logits\")\n",
    ")\n",
    "cluster_df.show(n=10, truncate=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "807f705c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+--------------------------------------------------+--------------------------------------------------+\n",
      "|dominant_cluster|                                        proba_list|                                 avg_probabilities|\n",
      "+----------------+--------------------------------------------------+--------------------------------------------------+\n",
      "|               1|[[4.9514707E-5, 3.681495E-5, 2.595474E-5, 1.529...|[2.362997E-5, 4.499179E-5, 4.3323707E-5, 1.6415...|\n",
      "|               2|[[4.5976332E-5, 1.9088793E-5, 2.4028224E-5, 8.9...|[2.4171377E-5, 2.2901053E-5, 1.4540791E-5, 7.44...|\n",
      "|               0|[[1.5335014E-5, 1.0890696E-5, 1.909136E-5, 6.78...|[5.1414485E-5, 1.0827277E-4, 1.0225503E-4, 6.50...|\n",
      "+----------------+--------------------------------------------------+--------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def avg_probabilities_udf(probabilities_list):\n",
    "    tensor = torch.tensor(probabilities_list)\n",
    "    mean_tensor = torch.mean(tensor, dim=0)\n",
    "    return mean_tensor.tolist()\n",
    "\n",
    "\n",
    "average_probabilities = F.udf(avg_probabilities_udf, ArrayType(FloatType()))\n",
    "\n",
    "# group and apply the UDF\n",
    "avg_probabilities_df = (\n",
    "    cluster_df.groupBy(\"dominant_cluster\")\n",
    "    .agg(F.collect_list(\"probabilities\").alias(\"proba_list\"))\n",
    "    .withColumn(\"avg_probabilities\", average_probabilities(col(\"proba_list\")))\n",
    ")\n",
    "avg_probabilities_df.show(n=10, truncate=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "938df852",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 27:>                                                         (0 + 2) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+--------------------------------------------------+\n",
      "|dominant_cluster|                        renormalized_probabilities|\n",
      "+----------------+--------------------------------------------------+\n",
      "|               1|[2.3629971E-5, 4.4991793E-5, 4.332371E-5, 1.641...|\n",
      "|               2|[2.4171377E-5, 2.2901053E-5, 1.4540791E-5, 7.44...|\n",
      "|               0|[5.1414485E-5, 1.0827277E-4, 1.0225503E-4, 6.50...|\n",
      "+----------------+--------------------------------------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# renormarlize the probabilities\n",
    "def renormalize_probabilities(probabilities):\n",
    "    probabilities_tensor = torch.tensor(probabilities)\n",
    "    probabilities_tensor /= torch.sum(probabilities_tensor)\n",
    "    return probabilities_tensor.tolist()\n",
    "\n",
    "\n",
    "renormalize_probabilities_udf = F.udf(renormalize_probabilities, ArrayType(FloatType()))\n",
    "avg_probabilities_df = avg_probabilities_df.withColumn(\n",
    "    \"renormalized_probabilities\", renormalize_probabilities_udf(\"avg_probabilities\")\n",
    ")\n",
    "avg_probabilities_df.select(\n",
    "    \"dominant_cluster\",\n",
    "    \"renormalized_probabilities\",\n",
    ").show(truncate=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6269283f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 44:======================================>                   (2 + 1) / 3]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probabilities saved to /storage/home/hcoda1/9/mgustineli3/p-dsgt_clef2025-0/shared/plantclef/data/embeddings/test_2025/test_2025_embed_probabilities_clustered\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# write the DataFrame to parquet\n",
    "output_path = f\"{data_path}/test_2025/test_2025_embed_probabilities_clustered\"\n",
    "probabilities_df = avg_probabilities_df.select(\n",
    "    \"dominant_cluster\",\n",
    "    \"renormalized_probabilities\",\n",
    ")\n",
    "probabilities_df.write.mode(\"overwrite\").parquet(output_path)\n",
    "print(f\"Probabilities saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "18174037",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of classes: 7806\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 32:>                                                         (0 + 2) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+--------------------------------------------------+\n",
      "|dominant_cluster|                             species_probabilities|\n",
      "+----------------+--------------------------------------------------+\n",
      "|               1|{1482309 -> 2.7289409E-5, 1356646 -> 3.14462E-5...|\n",
      "|               2|{1482309 -> 2.0991134E-5, 1356646 -> 3.1725456E...|\n",
      "|               0|{1482309 -> 3.854579E-5, 1356646 -> 7.4256466E-...|\n",
      "+----------------+--------------------------------------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StringType, MapType\n",
    "from plantclef.config import get_class_mappings_file\n",
    "\n",
    "class_mappings_file = get_class_mappings_file()\n",
    "with open(class_mappings_file) as f:\n",
    "    class_index_to_name = {i: line.strip() for i, line in enumerate(f)}\n",
    "\n",
    "num_classes = len(class_index_to_name)\n",
    "print(f\"Number of classes: {num_classes}\")\n",
    "\n",
    "\n",
    "def map_species_probabilities(probabilities, k=5):\n",
    "    probabilities_tensor = torch.tensor(probabilities)\n",
    "    top_probs, top_indices = torch.topk(probabilities_tensor, k=k)\n",
    "    top_probs = top_probs.cpu().numpy()\n",
    "    top_indices = top_indices.cpu().numpy()\n",
    "    result = {\n",
    "        class_index_to_name.get(index, \"Unknown\"): float(prob)\n",
    "        for index, prob in zip(top_indices, top_probs)\n",
    "    }\n",
    "    return result\n",
    "\n",
    "\n",
    "species_probabilities_udf = F.udf(\n",
    "    lambda probs: map_species_probabilities(probs, k=num_classes),\n",
    "    MapType(StringType(), FloatType()),\n",
    ")\n",
    "avg_probabilities_df = avg_probabilities_df.withColumn(\n",
    "    \"species_probabilities\", species_probabilities_udf(\"renormalized_probabilities\")\n",
    ")\n",
    "avg_probabilities_df.select(\"dominant_cluster\", \"species_probabilities\").show(\n",
    "    truncate=50\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9ce14030",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dominant_cluster</th>\n",
       "      <th>species_probabilities</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>{'1482309': 2.728940853558015e-05, '1356646': ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>{'1482309': 2.0991134078940377e-05, '1356646':...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>{'1482309': 3.8545789720956236e-05, '1356646':...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   dominant_cluster                              species_probabilities\n",
       "0                 1  {'1482309': 2.728940853558015e-05, '1356646': ...\n",
       "1                 2  {'1482309': 2.0991134078940377e-05, '1356646':...\n",
       "2                 0  {'1482309': 3.8545789720956236e-05, '1356646':..."
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# conver to pandas\n",
    "pandas_df = avg_probabilities_df.select(\n",
    "    \"dominant_cluster\",\n",
    "    \"species_probabilities\",\n",
    ").toPandas()\n",
    "pandas_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86a9da30",
   "metadata": {},
   "source": [
    "### write to csv for Bayesian prior inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fee4ac53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame written to /storage/home/hcoda1/9/mgustineli3/p-dsgt_clef2025-0/shared/plantclef/data/clustering/test_2025_embed_probabilities_clustered.csv\n"
     ]
    }
   ],
   "source": [
    "probabilities_df = pandas_df[\n",
    "    [\n",
    "        \"dominant_cluster\",\n",
    "        \"species_probabilities\",\n",
    "    ]\n",
    "]\n",
    "\n",
    "\n",
    "# write pandas to PACE as CSV\n",
    "def write_to_pace(df, path):\n",
    "    df.to_csv(path, index=False)\n",
    "    print(f\"DataFrame written to {path}\")\n",
    "\n",
    "\n",
    "# write the DataFrame to csv\n",
    "file_name = \"test_2025_embed_probabilities_clustered.csv\"\n",
    "output_path = f\"{root}/p-dsgt_clef2025-0/shared/plantclef/data/clustering/{file_name}\"\n",
    "write_to_pace(probabilities_df, output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6fba6d9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   dominant_cluster                              species_probabilities  \\\n",
      "0                 1  {'1482309': 2.728940853558015e-05, '1356646': ...   \n",
      "1                 2  {'1482309': 2.0991134078940377e-05, '1356646':...   \n",
      "2                 0  {'1482309': 3.8545789720956236e-05, '1356646':...   \n",
      "\n",
      "   cutoff_10  cutoff_20  cutoff_30  cutoff_40  cutoff_50  cutoff_60  \\\n",
      "0          4         14         39         97        219        453   \n",
      "1          2         14         43        112        245        486   \n",
      "2         11         40         96        197        386        729   \n",
      "\n",
      "   cutoff_70  cutoff_80  cutoff_90  cutoff_95  \n",
      "0        917       1806       3447       4825  \n",
      "1        921       1707       3164       4458  \n",
      "2       1362       2484       4349       5713  \n",
      "Mean species count per cumulative threshold:\n",
      "cutoff_10       5\n",
      "cutoff_20      22\n",
      "cutoff_30      59\n",
      "cutoff_40     135\n",
      "cutoff_50     283\n",
      "cutoff_60     556\n",
      "cutoff_70    1066\n",
      "cutoff_80    1999\n",
      "cutoff_90    3653\n",
      "cutoff_95    4998\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# define thresholds to analyze\n",
    "thresholds = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 0.95]\n",
    "\n",
    "\n",
    "# function to count species needed to reach each threshold\n",
    "def cumsum_species_counts(prob_dict, thresholds):\n",
    "    probs = np.array(sorted(prob_dict.values(), reverse=True))\n",
    "    cumsum = np.cumsum(probs)\n",
    "    return {\n",
    "        f\"cutoff_{int(t * 100)}\": np.searchsorted(cumsum, t) + 1  # +1 for species count\n",
    "        for t in thresholds\n",
    "    }\n",
    "\n",
    "\n",
    "# Apply the function across rows\n",
    "cumsum_df = pandas_df[\"species_probabilities\"].apply(\n",
    "    lambda row: pd.Series(cumsum_species_counts(row, thresholds))\n",
    ")\n",
    "\n",
    "# Combine with original DataFrame if needed\n",
    "result_df = pd.concat([pandas_df, cumsum_df], axis=1)\n",
    "\n",
    "# Example: show first few rows\n",
    "print(result_df.head())\n",
    "\n",
    "# Or compute the mean number of species needed at each threshold across the dataset\n",
    "mean_cutoffs = cumsum_df.mean().astype(int)\n",
    "print(\"Mean species count per cumulative threshold:\")\n",
    "print(mean_cutoffs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "897be77c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.044477567076683044"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "row = pandas_df[\"species_probabilities\"].iloc[0]\n",
    "max_val = max(row.values())\n",
    "max_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ed4513e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLUSTER: 1\n",
      "  Salix herbacea L.: 4.4%\n",
      "  Geum montanum L.: 2.4%\n",
      "  Festuca ovina L.: 2.3%\n",
      "  Carex curvula All.: 2.0%\n",
      "  Omalotheca supina (L.) DC.: 1.8%\n",
      "  Festuca nigrescens Lam.: 1.7%\n",
      "  Carex atrata L.: 0.9%\n",
      "  Salix serpillifolia Scop.: 0.9%\n",
      "  Veronica repens Clarion ex DC.: 0.7%\n",
      "  Scorzoneroides helvetica (Mérat) Holub: 0.7%\n",
      "\n",
      "CLUSTER: 2\n",
      "  Festuca ovina L.: 7.7%\n",
      "  Salix herbacea L.: 2.5%\n",
      "  Festuca quadriflora Honck.: 1.8%\n",
      "  Salix serpillifolia Scop.: 1.6%\n",
      "  Tephroseris integrifolia (L.) Holub: 1.0%\n",
      "  Asplenium cuneifolium Viv.: 0.8%\n",
      "  Festuca nigrescens Lam.: 0.8%\n",
      "  Veronica repens Clarion ex DC.: 0.7%\n",
      "  Oreochloa elegans (Sennen) A.W.Hill: 0.7%\n",
      "  Botrychium simplex E.Hitchc.: 0.6%\n",
      "\n",
      "CLUSTER: 0\n",
      "  Salicornia fruticosa (L.) L.: 2.0%\n",
      "  Thinopyrum junceum (L.) Á.Löve: 1.5%\n",
      "  Taeniatherum caput-medusae (L.) Nevski: 1.1%\n",
      "  Calamagrostis arenaria (L.) Roth: 0.9%\n",
      "  Medicago marina L.: 0.9%\n",
      "  Lotus creticus L.: 0.7%\n",
      "  Galium spurium L.: 0.7%\n",
      "  Bromus madritensis L.: 0.7%\n",
      "  Agrostis gigantea Roth: 0.6%\n",
      "  Arthrocaulon macrostachyum (Moric.) Piirainen & G.Kadereit: 0.5%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "species_metadata = \"~/p-dsgt_clef2025-0/shared/plantclef/data/species_metadata.csv\"\n",
    "species_df = pd.read_csv(species_metadata)\n",
    "\n",
    "\n",
    "# print top 10 species and probabilities for each dominant cluster\n",
    "def print_top_species_by_cluster(df, species_df, top_n=5):\n",
    "    for cluster in df[\"dominant_cluster\"].unique():\n",
    "        print(f\"CLUSTER: {cluster}\")\n",
    "        top_species = df[df[\"dominant_cluster\"] == cluster][\n",
    "            \"species_probabilities\"\n",
    "        ].iloc[0]\n",
    "        sorted_species = sorted(top_species.items(), key=lambda x: x[1], reverse=True)[\n",
    "            :top_n\n",
    "        ]\n",
    "        # filter species_df based on species_id\n",
    "        species_ids = [int(s[0]) for s in sorted_species]\n",
    "        species_df_merged = species_df[species_df[\"species_id\"].isin(species_ids)]\n",
    "        for species_id, prob in sorted_species:\n",
    "            species_name = species_df_merged[\n",
    "                species_df_merged[\"species_id\"] == int(species_id)\n",
    "            ][\"species\"].values[0]\n",
    "            print(f\"  {species_name}: {prob * 100:.1f}%\")\n",
    "        print()\n",
    "\n",
    "\n",
    "print_top_species_by_cluster(result_df, species_df, top_n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27c69059",
   "metadata": {},
   "source": [
    "### use prior with the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "302785b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_name</th>\n",
       "      <th>kmeans_cluster</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CBN-Pla-A1-20190814.jpg</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CBN-Pla-D6-20190814.jpg</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CBN-PdlC-C5-20140901.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LISAH-BOU-0-37-20230512.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CBN-Pla-E4-20130808.jpg</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    image_name  kmeans_cluster\n",
       "0      CBN-Pla-A1-20190814.jpg               2\n",
       "1      CBN-Pla-D6-20190814.jpg               2\n",
       "2     CBN-PdlC-C5-20140901.jpg               1\n",
       "3  LISAH-BOU-0-37-20230512.jpg               0\n",
       "4      CBN-Pla-E4-20130808.jpg               2"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dominant_cluster</th>\n",
       "      <th>species_probabilities</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>{'1482309': 2.728940853558015e-05, '1356646': ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>{'1482309': 2.0991134078940377e-05, '1356646':...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>{'1482309': 3.8545789720956236e-05, '1356646':...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   dominant_cluster                              species_probabilities\n",
       "0                 1  {'1482309': 2.728940853558015e-05, '1356646': ...\n",
       "1                 2  {'1482309': 2.0991134078940377e-05, '1356646':...\n",
       "2                 0  {'1482309': 3.8545789720956236e-05, '1356646':..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_cluster_csv = f\"{root}/p-dsgt_clef2025-0/shared/plantclef/data/clustering/test_2025_dominant_clusters.csv\"\n",
    "test_cluster_logits = f\"{root}/p-dsgt_clef2025-0/shared/plantclef/data/clustering/test_2025_embed_probabilities_clustered.csv\"\n",
    "\n",
    "test_cluster_df = pd.read_csv(test_cluster_csv)\n",
    "display(test_cluster_df.head())\n",
    "\n",
    "probabilities_df = pd.read_csv(test_cluster_logits)\n",
    "display(probabilities_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "32fea641",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    {'1482309': 2.728940853558015e-05, '1356646': ...\n",
       "1    {'1482309': 2.0991134078940377e-05, '1356646':...\n",
       "2    {'1482309': 3.8545789720956236e-05, '1356646':...\n",
       "Name: species_probabilities, dtype: object"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ast\n",
    "\n",
    "probabilities_df = probabilities_df[\"species_probabilities\"].apply(ast.literal_eval)\n",
    "probabilities_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e3776c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
