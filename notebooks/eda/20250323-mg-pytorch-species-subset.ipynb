{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train species subset for pytorch webinar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/03/23 15:50:09 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/03/23 15:50:09 WARN SparkConf: Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://atl1-1-03-007-5-0.pace.gatech.edu:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.5</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[4]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>clef</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x15553061ad10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from plantclef.spark import get_spark\n",
    "\n",
    "spark = get_spark(cores=4, memory=\"20g\")\n",
    "display(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Mar 23 03:50:11 PM EDT 2025\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Get list of stored filed in cloud bucket\n",
    "root = Path(os.path.expanduser(\"~\"))\n",
    "! date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- image_name: string (nullable = true)\n",
      " |-- path: string (nullable = true)\n",
      " |-- data: binary (nullable = true)\n",
      " |-- organ: string (nullable = true)\n",
      " |-- species_id: integer (nullable = true)\n",
      " |-- obs_id: long (nullable = true)\n",
      " |-- license: string (nullable = true)\n",
      " |-- partner: string (nullable = true)\n",
      " |-- author: string (nullable = true)\n",
      " |-- altitude: double (nullable = true)\n",
      " |-- latitude: double (nullable = true)\n",
      " |-- longitude: double (nullable = true)\n",
      " |-- gbif_species_id: string (nullable = true)\n",
      " |-- species: string (nullable = true)\n",
      " |-- genus: string (nullable = true)\n",
      " |-- family: string (nullable = true)\n",
      " |-- dataset: string (nullable = true)\n",
      " |-- publisher: string (nullable = true)\n",
      " |-- references: string (nullable = true)\n",
      " |-- url: string (nullable = true)\n",
      " |-- learn_tag: string (nullable = true)\n",
      " |-- image_backup_url: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+-----+----------+----------+--------------------+-------+----------------+--------+-----------------+------------------+---------------+--------------------+-------------+----------+--------+-----------+--------------------+--------------------+---------+--------------------+\n",
      "|          image_name|                path|                data|organ|species_id|    obs_id|             license|partner|          author|altitude|         latitude|         longitude|gbif_species_id|             species|        genus|    family| dataset|  publisher|          references|                 url|learn_tag|    image_backup_url|\n",
      "+--------------------+--------------------+--------------------+-----+----------+----------+--------------------+-------+----------------+--------+-----------------+------------------+---------------+--------------------+-------------+----------+--------+-----------+--------------------+--------------------+---------+--------------------+\n",
      "|0703b7eb2951080a5...|/PlantCLEF2024/tr...|[FF D8 FF E0 00 1...| leaf|   1737493|1014839418|            cc-by-sa|   NULL|   Rudi Kraševec|   596.0|45.74738691666667|14.416461944444444|      5371781.0|Selinum carvifoli...|      Selinum|  Apiaceae|plantnet|   plantnet|https://identify....|https://bs.plantn...|    train|https://lab.plant...|\n",
      "|7b435513acf8ff561...|/PlantCLEF2024/tr...|[FF D8 FF E0 00 1...|habit|   1397675|3327730052|http://creativeco...|   NULL|   Rick Williams|    NULL|             NULL|              NULL|      3107034.0|Senecio soldanell...|      Senecio|Asteraceae|    gbif|iNaturalist|https://www.inatu...|https://inaturali...|    train|https://lab.plant...|\n",
      "|82f8e37f6feb7f122...|/PlantCLEF2024/tr...|[FF D8 FF E0 00 1...| bark|   1741934|3456737412|http://creativeco...|   NULL|Marina Privalova|    NULL|             NULL|              NULL|      9540150.0|Silphiodaucus pru...|Silphiodaucus|  Apiaceae|    gbif|iNaturalist|https://www.inatu...|https://inaturali...|    train|https://lab.plant...|\n",
      "|adcc56d13a8a59326...|/PlantCLEF2024/tr...|[FF D8 FF E0 00 1...| bark|   1363733|1012013282|            cc-by-sa|   NULL|Manuel Hernández|    16.0|        39.477479|         -0.387182|      2965379.0| Medicago arborea L.|     Medicago|  Fabaceae|plantnet|   plantnet|https://identify....|https://bs.plantn...|    train|https://lab.plant...|\n",
      "|b22aad71e33d5d2cb...|/PlantCLEF2024/tr...|[FF D8 FF E0 00 1...|fruit|   1359627|1012041937|            cc-by-sa|   NULL|   dixie cousins|    NULL|             NULL|              NULL|      3152658.0|Gossypium herbace...|    Gossypium| Malvaceae|plantnet|   plantnet|https://identify....|https://bs.plantn...|    train|https://lab.plant...|\n",
      "+--------------------+--------------------+--------------------+-----+----------+----------+--------------------+-------+----------------+--------+-----------------+------------------+---------------+--------------------+-------------+----------+--------+-----------+--------------------+--------------------+---------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Path and dataset names\n",
    "data_path = f\"{root}/p-dsgt_clef2025-0/shared/plantclef/data/parquet/\"\n",
    "\n",
    "# Define the path to the train and test parquet files\n",
    "train_path = f\"{data_path}/train\"\n",
    "\n",
    "# Read the parquet files into a spark DataFrame\n",
    "train_df = spark.read.parquet(train_path)\n",
    "\n",
    "# Show the data\n",
    "train_df.printSchema()\n",
    "train_df.show(n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1395807,\n",
       " 1361281,\n",
       " 1394311,\n",
       " 1741880,\n",
       " 1397468,\n",
       " 1392407,\n",
       " 1397535,\n",
       " 1390793,\n",
       " 1392323,\n",
       " 1722440]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt_file_path = f\"{root}/clef/plantclef-2025/plantclef/train_species_ids.txt\"\n",
    "train_species_ids = []\n",
    "with open(txt_file_path, \"r\") as file:\n",
    "    for line in file:\n",
    "        species = int(line.replace(\"\\n\", \"\"))\n",
    "        train_species_ids.append(species)\n",
    "\n",
    "train_species_ids[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "42068"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# select subset of train data\n",
    "subset_df = train_df.where(F.col(\"species_id\").isin(train_species_ids))\n",
    "subset_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 5:====================================================>(2315 + 4) / 2330]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "|species_id|count|\n",
      "+----------+-----+\n",
      "|   1414366|  674|\n",
      "|   1722433|  625|\n",
      "|   1356576|  597|\n",
      "|   1412585|  594|\n",
      "|   1358613|  578|\n",
      "|   1414356|  558|\n",
      "|   1741903|  556|\n",
      "|   1360260|  551|\n",
      "|   1722522|  541|\n",
      "|   1722501|  532|\n",
      "|   1422217|  529|\n",
      "|   1414367|  512|\n",
      "|   1397475|  474|\n",
      "|   1397535|  462|\n",
      "|   1422218|  440|\n",
      "|   1357630|  440|\n",
      "|   1356286|  418|\n",
      "|   1419076|  415|\n",
      "|   1722625|  414|\n",
      "|   1392608|  410|\n",
      "+----------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# group by species_id\n",
    "grouped_df = subset_df.groupBy(\"species_id\").count().orderBy(F.desc(\"count\"))\n",
    "grouped_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(species_id=1391313, count=50),\n",
       " Row(species_id=1397463, count=50),\n",
       " Row(species_id=1392241, count=47),\n",
       " Row(species_id=1398772, count=46),\n",
       " Row(species_id=1392732, count=44),\n",
       " Row(species_id=1392323, count=42),\n",
       " Row(species_id=1393659, count=42),\n",
       " Row(species_id=1743968, count=36),\n",
       " Row(species_id=1420558, count=35),\n",
       " Row(species_id=1697384, count=35),\n",
       " Row(species_id=1651363, count=35),\n",
       " Row(species_id=1363722, count=31),\n",
       " Row(species_id=1741587, count=22),\n",
       " Row(species_id=1741661, count=19),\n",
       " Row(species_id=1390899, count=16),\n",
       " Row(species_id=1361275, count=15),\n",
       " Row(species_id=1399800, count=15),\n",
       " Row(species_id=1651485, count=12),\n",
       " Row(species_id=1580587, count=8),\n",
       " Row(species_id=1743474, count=2)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grouped_df.tail(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 16:===================================================>(2307 + 4) / 2330]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020\n",
      "root\n",
      " |-- image_name: string (nullable = true)\n",
      " |-- path: string (nullable = true)\n",
      " |-- data: binary (nullable = true)\n",
      " |-- organ: string (nullable = true)\n",
      " |-- species_id: integer (nullable = true)\n",
      " |-- obs_id: long (nullable = true)\n",
      " |-- license: string (nullable = true)\n",
      " |-- partner: string (nullable = true)\n",
      " |-- author: string (nullable = true)\n",
      " |-- altitude: double (nullable = true)\n",
      " |-- latitude: double (nullable = true)\n",
      " |-- longitude: double (nullable = true)\n",
      " |-- gbif_species_id: string (nullable = true)\n",
      " |-- species: string (nullable = true)\n",
      " |-- genus: string (nullable = true)\n",
      " |-- family: string (nullable = true)\n",
      " |-- dataset: string (nullable = true)\n",
      " |-- publisher: string (nullable = true)\n",
      " |-- references: string (nullable = true)\n",
      " |-- url: string (nullable = true)\n",
      " |-- learn_tag: string (nullable = true)\n",
      " |-- image_backup_url: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.window import Window\n",
    "\n",
    "truncate_rows = 10\n",
    "window_spec = Window.partitionBy(\"species_id\").orderBy(F.rand())\n",
    "subset_row_df = subset_df.withColumn(\"row_number\", F.row_number().over(window_spec))\n",
    "filtered_df = subset_row_df.filter(F.col(\"row_number\") <= truncate_rows).drop(\n",
    "    \"row_number\"\n",
    ")\n",
    "print(filtered_df.count())\n",
    "filtered_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered DataFrame saved to /storage/home/hcoda1/9/mgustineli3/p-dsgt_clef2025-0/shared/plantclef/data/parquet/train_pytorch_webinar_filtered\n"
     ]
    }
   ],
   "source": [
    "# write dataframe to parquet\n",
    "data_path = f\"{root}/p-dsgt_clef2025-0/shared/plantclef/data/parquet\"\n",
    "output_path = f\"{data_path}/train_pytorch_webinar_filtered\"\n",
    "# repartition the DataFrame into 20 partitions before writing to parquet\n",
    "filtered_df = filtered_df.repartition(20)\n",
    "filtered_df.write.mode(\"overwrite\").parquet(output_path)\n",
    "\n",
    "print(f\"Filtered DataFrame saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
