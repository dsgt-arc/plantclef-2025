{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.append(os.path.expanduser(\"~/clef/plantclef-2025/\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: PYSPARK_DRIVER_MEMORY=32g\n",
      "env: PYSPARK_EXECUTOR_MEMORY=16g\n",
      "env: SPARK_LOCAL_DIR=/tmp/spark-tmp\n"
     ]
    }
   ],
   "source": [
    "%env PYSPARK_DRIVER_MEMORY=32g\n",
    "%env PYSPARK_EXECUTOR_MEMORY=16g\n",
    "%env SPARK_LOCAL_DIR=/tmp/spark-tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/03/06 03:17:40 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/03/06 03:17:40 WARN SparkConf: Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://atl1-1-01-004-24-1.pace.gatech.edu:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.5</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[24]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>clef</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fffa672c310>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from plantclef.spark import get_spark\n",
    "\n",
    "spark = get_spark()\n",
    "display(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- image_name: string (nullable = true)\n",
      " |-- species_id: integer (nullable = true)\n",
      " |-- cls_embedding: array (nullable = true)\n",
      " |    |-- element: float (containsNull = true)\n",
      " |-- sample_id: integer (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+--------------------+---------+\n",
      "|          image_name|species_id|       cls_embedding|sample_id|\n",
      "+--------------------+----------+--------------------+---------+\n",
      "|3a2c58a78ee93b471...|   1363472|[0.9020945, 0.016...|       15|\n",
      "|0a0bf86d70307e8db...|   1361957|[-0.26025677, -0....|       15|\n",
      "|7990901729be71186...|   1363472|[-0.1633016, -0.0...|       15|\n",
      "|80257a4818f5955f9...|   1392612|[-0.7547744, 0.39...|       15|\n",
      "|e13e476d0dc36ed7b...|   1360562|[0.14017674, 0.05...|       15|\n",
      "+--------------------+----------+--------------------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "home = Path.home()\n",
    "train_embeddings_path = home / \"scratch/plantclef/data/embeddings/train/data\"\n",
    "\n",
    "train_df = spark.read.parquet(str(train_embeddings_path))\n",
    "train_df.printSchema()\n",
    "train_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "768"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "grouped_df = train_df.groupBy(\"species_id\") \\\n",
    "    .agg(F.collect_list(\"cls_embedding\").alias(\"embeddings_list\"))\n",
    "    \n",
    "len(train_df.select(\"cls_embedding\").first()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 3:=======================================================> (44 + 1) / 45]\r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType, StructField, IntegerType, ArrayType, FloatType\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import faiss\n",
    "\n",
    "\n",
    "num_centroids = 50\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"centroid_id\", IntegerType()),\n",
    "    StructField(\"species_id\", IntegerType()),\n",
    "    StructField(\"embedding\", ArrayType(FloatType()))\n",
    "])\n",
    "\n",
    "def compute_centroids(pdf):\n",
    "    \"\"\"Compute centroids for each species group using FAISS and return as rows\"\"\"\n",
    "    results = []\n",
    "    \n",
    "    embedding_dim = 768\n",
    "    \n",
    "    # res = faiss.StandardGpuResources()\n",
    "    \n",
    "    for species_id, group_pdf in pdf.groupby('species_id'):\n",
    "        all_embeddings = []\n",
    "        for row in group_pdf['embeddings_list']:\n",
    "            all_embeddings.extend(row)\n",
    "        \n",
    "        embeddings = np.vstack(all_embeddings).astype(np.float32)\n",
    "        \n",
    "        actual_centroids = min(num_centroids, len(embeddings))\n",
    "        \n",
    "        cpu_index = faiss.IndexFlatL2(embedding_dim)\n",
    "        # gpu_index = faiss.index_cpu_to_gpu(res, 0, cpu_index)\n",
    "        \n",
    "        kmeans = faiss.Clustering(embedding_dim, actual_centroids)\n",
    "        kmeans.niter = 20\n",
    "        kmeans.min_points_per_centroid = 1\n",
    "        kmeans.train(embeddings, cpu_index)\n",
    "        # kmeans.train(embeddings, gpu_index)\n",
    "        \n",
    "        centroids_array = faiss.vector_float_to_array(kmeans.centroids)\n",
    "        centroids_reshaped = centroids_array.reshape(actual_centroids, embedding_dim)\n",
    "        \n",
    "        for centroid_id in range(actual_centroids):\n",
    "            centroid_vector = centroids_reshaped[centroid_id].tolist()\n",
    "            results.append((centroid_id, species_id, centroid_vector))\n",
    "    \n",
    "    return pd.DataFrame(results, columns=[\"centroid_id\", \"species_id\", \"embedding\"])\n",
    "\n",
    "num_partitions = grouped_df.rdd.getNumPartitions()\n",
    "centroids_df = grouped_df.groupBy().applyInPandas(compute_centroids, schema).repartition(num_partitions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "output_path = home / f\"scratch/plantclef/data/parquet/train_centroids/num_centroids={num_centroids}\"\n",
    "centroids_df.write.mode(\"overwrite\").parquet(str(output_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 9:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+--------------------+\n",
      "|centroid_id|species_id|           embedding|\n",
      "+-----------+----------+--------------------+\n",
      "|          6|   1391964|[1.0990983, 0.259...|\n",
      "|          9|   1360169|[-0.5898131, 0.29...|\n",
      "|          5|   1389806|[0.5803415, -0.23...|\n",
      "|          3|   1393591|[-0.24192038, -0....|\n",
      "|          7|   1361603|[0.030800506, -0....|\n",
      "+-----------+----------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "centroids_df.show(5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
